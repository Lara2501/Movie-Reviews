{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "#from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "    \n",
    "short_pos = open(\"short_reviews/positive.txt\",\"r\").read()\n",
    "short_neg = open(\"short_reviews/negative.txt\",\"r\").read()\n",
    "\n",
    "# move this up here\n",
    "documents = []\n",
    "df = []\n",
    "\n",
    "#  j is adject, r is adverb, and v is verb\n",
    "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "allowed_word_types = [\"J\"]\n",
    "\n",
    "for p in short_pos.split('\\n'):\n",
    "    documents.append( (p, \"pos\") ) # Apenda os documentos com a variavel resposta\n",
    "    # Primeiro vou remover palavras que contem numeros\n",
    "    words = word_tokenize(re.sub(r'\\w*\\d\\w*', '', p).strip())\n",
    "    pos = nltk.pos_tag(words) # Coloca etiqueta do tipo de palavra\n",
    "    ww = []\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            ww.append(w[0].lower())\n",
    "    df.append( (' '.join(ww), \"pos\") )\n",
    "    \n",
    "for p in short_neg.split('\\n'):\n",
    "    documents.append( (p, \"neg\") ) # Apenda os documentos com a variavel resposta\n",
    "    # Primeiro vou remover palavras que contem numeros\n",
    "    words = word_tokenize(re.sub(r'\\w*\\d\\w*', '', p).strip())\n",
    "    pos = nltk.pos_tag(words) # Coloca etiqueta do tipo de palavra\n",
    "    ww = []\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            ww.append(w[0].lower())\n",
    "    df.append( (' '.join(ww), \"neg\") )\n",
    "    \n",
    "df = pd.DataFrame(df, columns = ['sentence', 'y_true'])\n",
    "\n",
    "save_documents = open(\"pickled_algos/documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()\n",
    "\n",
    "\n",
    "save_documents = open(\"pickled_algos/df.pickle\",\"wb\")\n",
    "pickle.dump(df, save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.1, random_state = 0, stratify = df['y_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 50 folds for each of 16 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend MultiprocessingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=5)]: Done 800 out of 800 | elapsed:  2.2min finished\n",
      "C:\\Users\\laran\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "seed = 123\n",
    "\n",
    "parameters = {\n",
    "    'transformer__tfidf__max_features': [1000],\n",
    "    #'transformer__tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'transformer__tfidf__ngram_range': [(1, 2)],\n",
    "    'transformer__tfidf__max_df': [.6],\n",
    "    'transformer__tfidf__min_df': [20],\n",
    "    'clf__max_iter': [5000],\n",
    "    'clf__C': [.1, 1, 10, 100],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__fit_intercept': [True],\n",
    "    'clf__class_weight': ['balanced', None],\n",
    "    #'clf__solver': ['lbfgs'],\n",
    "    'clf__random_state': [seed]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', ColumnTransformer([('tfidf', TfidfVectorizer(analyzer = 'word'), 'sentence')], remainder = 'passthrough')),\n",
    "    ('clf', LogisticRegression() )\n",
    "])\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 5, random_state = seed)\n",
    "gs = GridSearchCV(pipeline, parameters, cv = cv, scoring = 'f1_macro', n_jobs = 5, verbose = 1, refit = True)\n",
    "\n",
    "#Rodando GridSearch\n",
    "with parallel_backend('multiprocessing'):\n",
    "    gs.fit(train.drop(columns = 'y_true'), train['y_true'])\n",
    "\n",
    "save_classifier = open(\"pickled_algos/gs.pickle\",\"wb\")\n",
    "pickle.dump(gs, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_names_tfidf(X, column_tfidf, **params_tfidf):\n",
    "    tfidf = TfidfVectorizer(**params_tfidf)\n",
    "    tfidf.fit(X[column_tfidf])\n",
    "    tfidf_feature_names = {'tdidf__' + x for x in tfidf.get_feature_names()}\n",
    "    feature_names = list(tfidf_feature_names) + list(X.columns.drop(column_tfidf))\n",
    "    return feature_names\n",
    "\n",
    "featurenames = feature_names_tfidf(X = train.drop(columns = 'y_true'), column_tfidf = 'sentence',\n",
    "    max_df = list(gs.best_params_.values())[list(gs.best_params_).index('transformer__tfidf__max_df')],\n",
    "    min_df = list(gs.best_params_.values())[list(gs.best_params_).index('transformer__tfidf__min_df')],\n",
    "    max_features = list(gs.best_params_.values())[list(gs.best_params_).index('transformer__tfidf__max_features')],\n",
    "    ngram_range = list(gs.best_params_.values())[list(gs.best_params_).index('transformer__tfidf__ngram_range')]\n",
    ")\n",
    "\n",
    "k = gs.best_estimator_.named_steps['clf']\n",
    "Features = pd.DataFrame(k.coef_.tolist())\n",
    "Features.columns = featurenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__max_iter': 5000,\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__random_state': 123,\n",
       " 'transformer__tfidf__max_df': 0.6,\n",
       " 'transformer__tfidf__max_features': 1000,\n",
       " 'transformer__tfidf__min_df': 20,\n",
       " 'transformer__tfidf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtr = gs.best_estimator_.predict(train.drop(columns = 'y_true'))\n",
    "predte = gs.best_estimator_.predict(test.drop(columns = 'y_true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "664"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "407+257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6260543580131209"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "668/1067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>y_true</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>407</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>127</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "y_true  neg  pos\n",
       "row_0           \n",
       "neg     407  276\n",
       "pos     127  257"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(predte, test['y_true'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
